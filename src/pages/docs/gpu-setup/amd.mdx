---
title: AMD GPUs
description: Metal GPU support on Jan for llama.cpp
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---
import { Callout, Steps } from 'nextra/components'

# AMD GPUs

Jan is designed to support AMD GPUs on Windows and Linux through its Desktop app. The default inference engine for local AI is `llama.cpp`, utilizes a Vulkan backend to ensure compatibility with AMD GPUs.

## Pre-requisites
- You do not need to install additional drivers or software for AMD GPU as it's supported by default with Vulkan.
- The following AMD Radeon cards are confirmed to work:

| AMD GPU                |
|-----------------------|
| Radeon RX Vega series |
| Radeon RX 7000 series |
| Radeon RX 6000 series |
| Radeon RX 5000 series |
| Radeon Pro series     |
| Radeon 600 series     |
| Radeon 500 series     |
| Radeon 400 series     |
| Radeon 300 series     |
| Radeon 200 series     |

<Callout type='info'>
You can find more information about AMD GPUs, such as dGPUs [here](https://en.wikipedia.org/wiki/Category:AMD_graphics_cards).
</Callout>


## Enable AMD Radeon RX GPU
To enable the use of AMD GPU in the Jan app, follow the steps below:
1. Open Jan application.
2. Go to **Settings** -> **Advanced Settings** -> **Accelerator** -> Enable and choose the Radeon RX GPU you want.
3. Navigate to the **Threads** tab.
4. Create a new chat.
5. Select a model size based on your hardware and the **recommended** tag for **VRAM** in the Hub. Expect the following outcomes:
    - High time to first token (ms)
    - Low throughput (tokens/sec)
