---
title: NVIDIA GPUs
description: Metal GPU support on Jan for llama.cpp
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---
import { Callout, Steps } from 'nextra/components'

# NVIDIA GPUs

Jan supports NVIDIA GPUs exclusively on Windows and Linux through its Desktop app. The default local AI inference engine, `llama.cpp`, has a CUBLAS backend.
## Pre-requisites
To use NVIDIA GPU, you must have a compatible NVIDIA GPU and the latest NVIDIA driver installed.
### NVIDIA Driver

- Install an [NVIDIA Driver](https://www.nvidia.com/Download/index.aspx) supporting CUDA 11.7 or higher.
    - Use the following command to verify the installation:

```bash
nvidia-smi
```

### CUDA Toolkit

- Install a [CUDA toolkit](https://developer.nvidia.com/cuda-downloads) that is compatible with your NVIDIA driver.
    - Use the following command to verify the installation:

```bash
nvcc --version
```
### Linux Specifics

- Ensure that `gcc-11`, `g++-11`, `cpp-11`, or higher is installed. To install, please see [here](https://gcc.gnu.org/projects/cxx-status.html#cxx17) for Ubuntu installation.

- **Post-Installation Actions**: Add CUDA libraries to `LD_LIBRARY_PATH`. To install, follow the [Post-installation Actions](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions) instructions.

### Validated NVIDIA Cards
The following NVIDIA cards are confirmed to work:

| NVIDIA GPU      |
|--------------------------|
| GeForce RTX 40 Series    |
| GeForce RTX 30 Series    |
| GeForce RTX 20 Series    |
| GeForce GTX Series       |
| NVIDIA Titan Series      |
| NVIDIA Quadro Series     |
| NVIDIA Tesla Series (Server GPUs) |


## Enable NVIDIA GPU
To enable the use of NVIDIA GPU in the Jan app, follow the steps below:
1.  Open Jan application
2.  Go to **Settings** -> **Advanced Settings** -> **Accelerator** -> Enable and choose the NVIDIA GPU you want.
3. Navigate to the **Threads** tab.
4. Create a new chat.
5. Select a model size based on your hardware and the **recommended** tag for **VRAM** in the Hub. Expect the following outcomes:
    - High time to first token (ms)
    - Low throughput (tokens/sec)