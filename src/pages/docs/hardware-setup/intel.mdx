---
title: Intel Arc GPU
description: Metal GPU support on Jan for llama.cpp
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---
import { Callout, Steps } from 'nextra/components'

# Intel Arc GPU

Jan supports Intel Arc on Windows and Linux via its Desktop app, functioning similarly to its AMD GPU support. The default local AI inference engine, `llama.cpp`, uses a Vulkan backend.

## Pre-requisites
You do not need to install additional drivers or software for Intel Arc GPU as it's supported by default with Vulkan.

<Callout type='info'>
You can find more information about Intel Arc GPU for Desktop/ Laptop as dGPU [here](https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html)
</Callout>

## Steps to enable Intel Arc GPU
To enable the use of Intel Arc GPU in the Jan app, follow the steps below:

1. Open Jan application
2. Go to **Settings** -> **Advanced Settings** -> **Accelerator** -> Enable and choose the Intel Arc GPU you want.
3. Navigate to the **Threads** tab.
4. Create a new chat.
5. Select a model size based on your hardware and the **recommended** tag for **VRAM** in the Hub. Expect the following outcomes:
    - High time to first token (ms)
    - Low throughput (tokens/sec)
