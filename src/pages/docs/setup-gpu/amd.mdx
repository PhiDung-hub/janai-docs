---
title: AMD GPU
description: Metal GPU support on Jan for llama.cpp
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---

# AMD GPU
- Jan supports AMD GPU on Windows and Linux only with Desktop app.
- Jan uses `llama.cpp` as the default inference engine for local  AI, which supports AMD GPU with Vulkan backend.
- You do not need to install any additional drivers or software for AMD GPU as it's supported by default with Vulkan.
- You can find more information about AMD GPU for as dGPU in [here](https://en.wikipedia.org/wiki/Category:AMD_graphics_cards)
- The validated cards that work:
  - Radeon RX Vega series
  - Radeon RX 7000 series
  - Radeon RX 6000 series
  - Radeon RX 5000 series
  - Radeon Pro
  - Radeon 600 series
  - Radeon 500 series
  - Radeon 400 series
  - Radeon 300 series
  - Radeon 200 series

## Steps to enable AMD Radeon RX GPU:
- Open Jan application
- Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the Radeon RX GPU that you want.
- Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
    - High time to first token (ms)
    - Low throughput (tokens/sec)

## WIP
There are several `backend` support from AMD that we are still monitoring and testing for Jan before releasing
  - [Blas support on HIP-supported AMD GPU with RoCM](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#hipblas)