---
title: TensorRT-LLM
description: A step-by-step guide on customizing the TensorRT-LLM extension.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    TensorRT-LLM Extension,
    TensorRT,
    tensorRT,
    extension,
  ]
---

import { Callout, Steps } from 'nextra/components'

# TensorRT-LLM

## Overview

Users with Nvidia GPUs can get **20-40% faster token generation speeds** than `llama.cpp` engine on their laptop or desktops by using [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). The more significant implication is that you are running FP16, which is more accurate than quantized models.

## TensortRT-LLM

This guide walks you through installing Jan's official [TensorRT-LLM Extension](https://github.com/janhq/nitro-tensorrt-llm). This extension uses [Nitro-TensorRT-LLM](https://github.com/janhq/nitro-tensorrt-llm) as the AI engine, instead of the default [Nitro-Llama-CPP](https://github.com/janhq/nitro).  It includes an efficient C++ server that executes the [TRT-LLM C++ runtime](https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html) natively. It also includes features and performance improvements like OpenAI compatibility, tokenizer improvements, and queues.

<Callout type='warning' emoji="">
  - This feature is only available for Windows users, Linux is coming soon.

  - Additionally, we only prebuilt a few demo models. You can always build your desired models directly on your machine. For more information, please see [here](#build-your-own-tensorrt-models).
</Callout>

### Pre-requisites

- A Windows PC
- Nvidia GPU(s): Ada or Ampere series (i.e. RTX 4000s & 3000s). More will be supported soon.
- 3GB+ of disk space to download TRT-LLM artifacts and a Nitro binary
- Jan v0.4.9+ or Jan v0.4.8-321+ (nightly)
- Nvidia Driver v535+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
- CUDA Toolkit v12.2+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))

<Callout type='warning'>
  If you are using our nightly builds, you may have to reinstall the TensorRT-LLM extension each time you update the app. We're working on better extension lifecycles - stay tuned.
</Callout>

<Steps>

### Step 1: Install TensorRT-Extension

1. Go to **Settings** > **Extensions**.
2. Select the TensorRT-LLM Extension and click the **Install** button.
<br/>
![Install Extension](../_assets/install-tensor.gif)
<br/>
3. Check that files are correctly downloaded.

```bash
ls ~/jan/extensions/@janhq/tensorrt-llm-extension/dist/bin
# Your Extension Folder should now include `nitro.exe`, among other artifacts needed to run TRT-LLM
```

### Step 2: Download a Compatible Model

TensorRT-LLM can only run models in `TensorRT` format. These models, aka "TensorRT Engines", are prebuilt for each target OS+GPU architecture.

We offer a handful of precompiled models for Ampere and Ada cards that you can immediately download and play with:

1. Restart the application and go to the Hub.
2. Look for models with the `TensorRT-LLM` label in the recommended models list > Click **Download**.

<Callout type='info'>
  This step might take some time. üôè
</Callout>

![image](https://hackmd.io/_uploads/rJewrEgRp.png)

3. Click Use and start chatting!
4. You may need to allow Nitro in your network.

![Nitro Network](../_assets/nitro-network.png)

### Step 3: Configure Settings

1. Navigate to the Thread section.
2. Select the model that you have downloaded.
3. Customize the default parameters of the model for how Jan runs TensorRT-LLM.
<Callout type='info'>
  Please see [here](/docs/models/model-parameters) for more detailed model parameters.
</Callout>
<br/>
![Configure Model](../_assets/set-tensor.gif)

</Steps>

## Disable Extension

To disable the extension, follow the steps below:

1. Navigate to the **Settings** > **Advanced Settings**.
2. On the **Jan Data Folder** click the **folder icon (üìÇ)** to access the data folder.
3. Navigate to the `~/jan/extensions` folder.
4. Open the `extensions.json` and change the `_active` value of the TensorRT-LLM to `false`
5. Restart the app to see that the TensorRT-LLM settings page has been removed.
<br/>
![Disable Extension](../_assets/disable-tensor.gif)


## Uninstall Extension

To uninstall the extension, follow the steps below:

1. Quit the app.
2. Navigate to the **Settings** > **Advanced Settings**.
3. On the **Jan Data Folder** click the **folder icon (üìÇ)** to access the data folder.
4. Navigate to the `~/jan/extensions/@janhq` folder.
5. Delete the **tensorrt-llm-extension** folder.
4. Reopen the app.
<br/>
![Delete Extension](../_assets/delete-tensor.gif)

## Troubleshooting

### Incompatible Extension vs Engine versions

For now, the model versions are pinned to the extension versions.

### Install Nitro-TensorRT-LLM Manually

You can reference the source code to manually build the artifacts needed to run the server and TensorRT-LLM. [Read here](https://github.com/janhq/nitro-tensorrt-llm?tab=readme-ov-file#quickstart).