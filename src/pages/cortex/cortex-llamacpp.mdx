---
title: Cortex.llamacpp
description: Cortex.llamacpp Architecture
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Cortex,
    Jan,
    LLMs
  ]
---

import { Callout, Steps } from 'nextra/components'
import { Cards, Card } from 'nextra/components'

<Callout type="warning">
ðŸš§ Cortex is under construction.
</Callout>
# Cortex.llamacpp

This guide walks you through how Cortex.llamacpp is designed, the codebase structure, and future plans.

If you use Cortex server, llamacpp is bundled by default and you donâ€™t need this guide. Conversely, if you are interested to use llamacpp directly, here are some unofficial docs that might help.

## Usage

See [Quickstart](/cortex/quickstart)

## Interface

Cortex.llamacpp has the following Interfaces:

- **HandleChatCompletion:** Processes chat completion tasks
    
    ```cpp
    void HandleChatCompletion(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **HandleEmbedding:** Generates embeddings for the input data provided
    
    ```cpp
    void HandleEmbedding(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **LoadModel:** Loads a model based on the specifications
    
    ```cpp
    void LoadModel(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **UnloadModel:** Unloads a model as specified
    
    ```cpp
    void UnloadModel(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **GetModelStatus:** Retrieves the status of a model
    
    ```cpp
    void GetModelStatus(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    

**Parameters:**

- **`jsonBody`**: The request content in JSON format.
- **`callback`**: A function that handles the response

## Architecture

Interaction between components to provide an API for `embedding` and `inference` tasks using the **`llama.cpp`** library.

These are main components:

- `cortex.cpp`: responsible for handling API requests and responses.
- `cortex.llamacpp`: makes APIs accessible through an Engine Interface, allowing others to easily use its features.
- **`enginei` i**mplements detailed logic for all **`llama engine`** APIs, handling endpoint logic and facilitating communication between **`cortex.cpp`** and **`llama engine`**.
- **`llama engine` :** exposes APIs for embedding and inference. It loads and unloads models and simplifies API calls to **`llama.cpp`**.
- **`llama.cpp` : a** submodule from the **`llama.cpp`** repository that provides the core functionality for embeddings and inferences.
- **`llama server context` :** a wrapper offers a simpler and more user-friendly interface for **`llama.cpp`** APIs

![Cortex llamacpp architecture](./_assets/cortex-llamacpp-arch.png)

### Activity architecture

- Cortex.llamacpp has 2 communication methods:
    - Streaming: When streaming is enabled, the response is processed and returned as chunks of results. This is done by creating tokens through inferencing, one token at a time.
    - Non-streaming: the response is processed as a whole. After **`llama server context`** completes the entire process, it returns a single result back to **`cortex.cpp`**.

![Cortex llamacpp architecture](./_assets/cortex-llamacpp-act.png)

## Code Structure

```
.
â”œâ”€â”€ base                              # Engine interface definition
|   â””â”€â”€ cortex-common                 # Common interfaces used for all engines
|      â””â”€â”€ enginei.h                  # Define abstract classes and interface methods for engines
â”œâ”€â”€ examples                          # Server example to integrate engine
â”‚   â””â”€â”€ server.cc                     # Example server demonstrating engine integration
â”œâ”€â”€ llama.cpp                         # Upstream llama.cpp repository
â”‚   â””â”€â”€ (files from upstream llama.cpp)
â”œâ”€â”€ src                               # Source implementation for llama.cpp
â”‚   â”œâ”€â”€ chat_completion_request.h     # OpenAI compatible request handling
â”‚   â”œâ”€â”€ llama_client_slot             # Manage vector of slots for parallel processing
â”‚   â”œâ”€â”€ llama_engine                  # Implementation llamacpp engine of model loading and inference 
â”‚   â”œâ”€â”€ llama_server_context          # Context management for chat completion requests
â”‚   â”‚   â”œâ”€â”€ slot                      # Struct for slot management
â”‚   â”‚   â””â”€â”€ llama_context             # Struct for llama context management
|   |   â””â”€â”€ chat_completion           # Struct for chat completion management
|   |   â””â”€â”€ embedding                 # Struct for embedding management
â”œâ”€â”€ third-party                       # Dependencies of the cortex.llamacpp project
â”‚   â””â”€â”€ (list of third-party dependencies)
```

## Runtime

## Roadmap
