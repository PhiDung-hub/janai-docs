---
title: Cortex
description: Cortex is an Local LLM engine for developers
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Discord integration,
    Discord,
    bot,
  ]
---

import { Callout, Steps } from 'nextra/components'

# Cortex

## About

![Cortex Cover Image](./_assets/cortex-cover.png)

Cortex is a Local LLM server that developers can use to build LLM apps. It can be used as a [standalone server](/cortex/usage/server), or [imported as a library](/cortex/usage/library). 

Cortex currently supports two inference engines: 
- [llama.cpp](/cortex/engines/llama.cpp)
- [TensorRT-LLM](/cortex/engines/tensorrt-llm)

<Callout>
  **Real-world Use**: Cortex powers [Jan](/docs), our local ChatGPT-alternative. 
  
  Cortex has been battle-tested through 800k downloads, and handles a variety of hardware and software edge cases.
</Callout>

### Roadmap

Cortex's [roadmap](/cortex/roadmap) is to implement an [OpenAI-equivalent API](https://platform.openai.com/docs/api-reference) using a fully open source stack. Our goal is to make switching to open source AI as easy as possible for developers.

### Architecture

Cortex's [architecture](/cortex/architecture) features C++ inference core, with [higher-order features](/cortex/architecture) handled in Typescript. 

Our [long-term direction](/cortex/roadmap) is to (eventually) move towards being a full C++ library to enable embedded and robotics use cases.