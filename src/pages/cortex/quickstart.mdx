---
title: Quickstart
description: Cortex Quickstart.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Cortex,
    Jan,
    LLMs
  ]
---

import { Callout, Steps } from 'nextra/components'
import { Cards, Card } from 'nextra/components'

# Quickstart

<Callout type="warning">
ðŸš§ Cortex is under construction.
</Callout>

To get started, ensure minimal [hardware requirements](/cortex/hardware), and follow the steps below:

```bash
npm i -g @janhq/cortex

# Initialize a compatible engine
cortex init

# Download a GGUF model
cortex models pull llama3

cortex models run llama3
```

You can also run Cortex in OpenAI-compatible server mode:
```bash
cortex serve
```

Visit the API playground at: http://localhost:1337/api 

And that's it!

## Best Practices

### Model Registry

By default, models download from Cortex's [model registry](https://huggingface.co/janhq) on Hugging Face. We include configurations for popular models. This is the easiest way to import new models. 

```bash
# cortex pull repo_name
cortex pull llama3
```

Available models include [llama3](https://huggingface.co/janhq/llama3), [mistral](https://huggingface.co/janhq/mistral), [tinyllama](https://huggingface.co/janhq/tinyllama), and [many more](https://huggingface.co/janhq).

You can also download `size`, `format`, and `quantization` variants of each model.

```bash
# cortex pull repo_name:branch_name
cortex pull llama3:7b
cortex pull llama3:8b-instruct-v3-gguf-Q4_K_M
cortex pull llama3:8b-instruct-v3-tensorrt-llm
```

Model variants are provided via the `branches` in each model's Hugging Face repo.

### Hugging Face Hub

You can download any model from Hugging Face. 

Depending on the model architecture, you may need to separately configure the runtime parameters.

```bash
# cortex pull org_name/repo_name
cortex models pull janhq/meta-llama-3-8b-instruct-GGUF
```
