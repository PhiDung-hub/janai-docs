---
title: TensorRT-LLM
description: A step-by-step guide on customizing the TensorRT-LLM extension.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    TensorRT-LLM Extension,
    TensorRT,
    tensorRT,
    extension,
  ]
---

import { Callout, Steps } from 'nextra/components'

# TensorRT-LLM
<Callout type="info">
This page is still under construction.
</Callout>
## Overview

Users with Nvidia GPUs can get **20-40% faster token generation speeds** than `llama.cpp` engine on their laptop or desktops by using [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). The more significant implication is that you are running FP16, which is more accurate than quantized models.

### Incompatible Extension vs Engine versions

For now, the model versions are pinned to the extension versions.

### Install Nitro-TensorRT-LLM Manually

You can reference the source code to manually build the artifacts needed to run the server and TensorRT-LLM. [Read here](https://github.com/janhq/nitro-tensorrt-llm?tab=readme-ov-file#quickstart).